# -*- coding: utf-8 -*-
"""Text Summarization using tfidf and fft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ncpNSO0nSLjdNA-yhF19wO-W8mC1P_bp
"""

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

mkdir -p /root/.kaggle/

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download pariza/bbc-news-summary

! unzip bbc-news-summary.zip

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

articles_path = "/content/bbc news summary/BBC News Summary/News Articles"
summaries_path = "/content/bbc news summary/BBC News Summary/Summaries"
categories_list = os.listdir(articles_path)

import glob as glob


def read_file(articles_path, summaries_path, categories_list, encoding = "ISO-8859-1"):
    articles = []
    summaries = []
    categories = []
    for category in categories_list:
        article_paths = glob.glob(os.path.join(articles_path, category, '*.txt'), recursive = True)
        summary_paths = glob.glob(os.path.join(summaries_path, category, '*.txt'), recursive = True)

        print(f'found {len(article_paths)} file in articles/{category} folder, {len(summary_paths)} file in summaries/{category}')

        if len(article_paths) != len(summary_paths):
            print("number of files is not equal")
            return
        for file in range(len(article_paths)):
            categories.append(category)
            with open(article_paths[file], mode = 'r', encoding = encoding ) as files:
                articles.append(files.read())
            with open(summary_paths[file], mode = 'r', encoding = encoding) as files:
                summaries.append(files.read())

    print(f'total {len(articles)} file in articles folder and {len(summaries)} files in summaries folder')
    return articles, summaries, categories

articles, summaries, categories = read_file(articles_path, summaries_path, categories_list)

data = pd.DataFrame({'articles': articles, 'summaries': summaries,
                    'categories': categories})

data.head()

data.isnull().sum()

import re
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text

def sentence_tokenizer(article):
    sentences = re.split(r'(?<=[.!?])\s+', article)
    return sentences
preprocessed_sentences = []
for article in data['articles']:
    preprocessed_article = preprocess_text(article)
    sentences = sentence_tokenizer(preprocessed_article)
    preprocessed_sentences.extend(sentences)
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(data['articles'])
print(tfidf_matrix)



import numpy as np
count = 0
modified_articles = []
with open('modified_articles.txt', 'w', encoding='utf-8') as file:
  for i, article in enumerate(data['articles']):
      sentences = sentence_tokenizer(article)
      article_sentences = []
      modified_article = []
      sentence_sums = []
      for sentence in sentences:
          sentence_scores = []

          for word in sentence.split():
              if word in tfidf_vectorizer.vocabulary_:
                  word_index = tfidf_vectorizer.vocabulary_[word]
                  word_scores = tfidf_matrix[:, word_index].toarray()
                  sentence_scores.extend(word_scores)

          if sentence_scores:
              sentence_scores = np.array(sentence_scores)
              power_spectrum = np.abs(np.fft.fft(sentence_scores))**2
              sentence_sum = np.sum(power_spectrum)
              sentence_sums.append(sentence_sum)
      sorted_indices = np.argsort(sentence_sums)[::-1]

      selected_indices = sorted_indices[:11]

      for index in selected_indices:
          selected_sentence = sentences[index]
          modified_article.append(selected_sentence)

      modified_articles.append(modified_article)
      count += len(modified_article)

      file.write("Article {}:\n".format(i+1))
      for sentence in modified_article:
          file.write(sentence + "\n")

  '''          if sentence_sum > 0.5:
                  article_sentences.append((sentence, sentence_sum))
                  modified_article.append(sentence)
          modified_article_str = " ".join(modified_article)
          modified_articles.append(modified_article_str)
  '''

import numpy as np
import time

count = 0
modified_articles = []
sentence_processing_times = []

with open('modified_articles.txt', 'w', encoding='utf-8') as file, open('sentence_processing_times.txt', 'w', encoding='utf-8') as time_file:
    for i, article in enumerate(data['articles']):
        sentences = sentence_tokenizer(article)
        article_sentences = []
        modified_article = []
        sentence_sums = []

        for sentence in sentences:
            start_time = time.time()  # Start measuring time for processing the current sentence

            sentence_scores = []
            for word in sentence.split():
                if word in tfidf_vectorizer.vocabulary_:
                    word_index = tfidf_vectorizer.vocabulary_[word]
                    word_scores = tfidf_matrix[:, word_index].toarray()
                    sentence_scores.extend(word_scores)
            if sentence_scores:
                sentence_scores = np.array(sentence_scores)
                power_spectrum = np.abs(np.fft.fft(sentence_scores)) ** 2
                sentence_sum = np.sum(power_spectrum)
                sentence_sums.append(sentence_sum)

            end_time = time.time()  # Stop measuring time for processing the current sentence
            processing_time = end_time - start_time
            sentence_processing_times.append(processing_time)

            # Write the sentence index and time taken to the 'sentence_processing_times.txt' file
            time_file.write("Sentence {}: Time taken: {} seconds\n".format(len(sentence_processing_times), processing_time))

        sorted_indices = np.argsort(sentence_sums)[::-1]
        selected_indices = sorted_indices[:10]
        for index in selected_indices:
            selected_sentence = sentences[index]
            modified_article.append(selected_sentence)

        modified_articles.append(modified_article)
        count += len(modified_article)

        file.write("Article {}:\n".format(i + 1))
        for sentence in modified_article:
            file.write(sentence + "\n")

# Calculate the average processing time per sentence
average_sentence_processing_time = sum(sentence_processing_times) / len(sentence_processing_times)
print("Average processing time per sentence:", average_sentence_processing_time)

minimum_sentence_count = float('inf')
article_number = None

for i, article in enumerate(data['articles']):
    sentences = sentence_tokenizer(article)
    sentence_count = len(sentences)

    if sentence_count < minimum_sentence_count:
        minimum_sentence_count = sentence_count
        article_number = i + 1

print(f"Minimum number of sentences in article {article_number}: {minimum_sentence_count}")

total_article_count = len(data['articles'])
total_sentence_count = 0

for article in data['articles']:
    sentences = sentence_tokenizer(article)
    total_sentence_count += len(sentences)

average_sentence_count = total_sentence_count / total_article_count

print(f"Average number of sentences per article: {average_sentence_count}")

pip install rouge

pip install rouge-score

from rouge_score import rouge_scorer

import sys
from rouge import Rouge
ref_summaries = data['summaries']
#sys.setrecursionlimit(10**6)

rouge = Rouge()

total_scores = {}

total_score_r1=0
num_articles = len(modified_articles)
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
total_score_p_r1=0
total_score_p_r2=0
total_score_p_rl=0
total_score_r_r1=0
total_score_r_r2=0
total_score_r_rl=0
total_score_f_r1=0
total_score_f_r2=0
total_score_f_rl=0
for i in range(num_articles):
    modified_article = str(modified_articles[i])
    reference_summary = str(ref_summaries[i])


    scores = scorer.score(modified_article, reference_summary)
    p_r1, r_r1, f_r1=scores['rouge1']
    p_r2, r_r2, f_r2=scores['rouge2']
    p_rl, r_rl, f_rl=scores['rougeL']

    total_score_p_r1+=p_r1
    total_score_p_r2+=p_r2
    total_score_p_rl+=p_rl
    total_score_r_r1+=r_r1
    total_score_r_r2+=r_r2
    total_score_r_rl+=r_rl
    total_score_f_r1+=f_r1
    total_score_f_r2+=f_r2
    total_score_f_rl+=f_rl

average_scores_p_r1=total_score_p_r1/num_articles
average_scores_p_r2=total_score_p_r2/num_articles
average_scores_p_rl=total_score_p_rl/num_articles
average_scores_r_r1=total_score_r_r1/num_articles
average_scores_r_r2=total_score_r_r2/num_articles
average_scores_r_rl=total_score_r_rl/num_articles
average_scores_f_r1=total_score_f_r1/num_articles
average_scores_f_r2=total_score_f_r2/num_articles
average_scores_f_rl=total_score_f_rl/num_articles

##average_scores = {}
##for rouge_type in total_scores.keys():
##    average_scores[rouge_type] = {
##        metric: total_scores[rouge_type][metric] / num_articles
##        for metric in total_scores[rouge_type].keys()
##    }
print("Average ROUGE scores for all articles:")
print(average_scores_p_r1)
print(average_scores_p_r2)
print(average_scores_p_rl)
print(average_scores_r_r1)
print(average_scores_r_r2)
print(average_scores_r_rl)
print(average_scores_f_r1)
print(average_scores_f_r2)
print(average_scores_f_rl)

from rouge import Rouge
reference_summaries = data['summaries']
rouge = Rouge()
scores = []

for i, modified_article in enumerate(modified_articles):
    reference_summary = reference_summaries[i]

    modified_article_str = ' '.join(modified_article)
    reference_summary_str = reference_summary

    score = rouge.get_scores(modified_article_str, reference_summary_str)
    scores.append(score)
for i, score in enumerate(scores):
    print(f"Article {i+1}:")
    print("ROUGE-1: ", score[0]["rouge-1"])
    print("ROUGE-2: ", score[0]["rouge-2"])
    print("ROUGE-L: ", score[0]["rouge-l"])
    print()

import nltk

from nltk.translate.bleu_score import sentence_bleu

candidate_summaries = modified_articles
reference_summaries = data['summaries']

with open('new_bleu_scores.txt', 'w') as file:
    for i, (candidate_summary, reference_summary) in enumerate(zip(candidate_summaries, reference_summaries)):
        candidate_summary_str = ' '.join(candidate_summary)
        reference_summary_str = ' '.join(reference_summary)
        candidate_summary_words = candidate_summary_str.split()
        reference_summary_words = reference_summary_str.split()


        candidate_summary_str = candidate_summary_str.lower()
        reference_summary_str = reference_summary_str.lower()

        candidate_tokens = nltk.word_tokenize(candidate_summary_str)
        reference_tokens = nltk.word_tokenize(reference_summary_str)

        bleu_1 = nltk.translate.bleu_score.sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0))
        bleu_2 = nltk.translate.bleu_score.sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0))
        bleu_3 = nltk.translate.bleu_score.sentence_bleu([reference_tokens], candidate_tokens, weights=(0.33, 0.33,0.33, 0))
        bleu_4 = nltk.translate.bleu_score.sentence_bleu([reference_tokens], candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25))


        file.write("Article #" + str(i+1) + "\n")
        file.write("BLEU-1: " + str(bleu_1) + "\n")
        file.write("BLEU-2: " + str(bleu_2) + "\n")
        file.write("BLEU-3: " + str(bleu_3) + "\n")
        file.write("BLEU-4: " + str(bleu_4) + "\n")
        file.write("--------------------------------------\n")

output_file = "new_rouge_scores.txt"
with open(output_file, 'w') as file:
  for i, score in enumerate(scores):
        file.write(f"ROUGE scores for article {i+1}:\n")
        file.write(str(score))
        file.write("\n\n")

from nltk.translate.bleu_score import sentence_bleu

candidate_summaries = modified_articles
reference_summaries = data['summaries']

with open('bleu_scores.txt', 'w') as file:
    for i, (candidate_summary, reference_summary) in enumerate(zip(candidate_summaries, reference_summaries)):
        candidate_summary_words = candidate_summary.split()
        reference_summary_words = reference_summary.split()
        bleu_score = sentence_bleu([reference_summary_words], candidate_summary_words)
        file.write(f"BLEU Score for article {i+1}: {bleu_score}\n")

from nltk.translate.bleu_score import sentence_bleu

candidate_summaries = modified_articles
reference_summaries = data['summaries']

total_bleu_score = 0
num_articles = len(candidate_summaries)

for i, (candidate_summary, reference_summary) in enumerate(zip(candidate_summaries, reference_summaries)):
    candidate_summary_words = " ".join(candidate_summary).split()
    reference_summary_words = reference_summary.split()
    bleu_score = sentence_bleu([reference_summary_words], candidate_summary_words)
    total_bleu_score += bleu_score

average_bleu_score = total_bleu_score / num_articles

print("Average BLEU Score:", average_bleu_score)

import sys
from rouge import Rouge

sys.setrecursionlimit(10**6)

rouge = Rouge()

total_scores = {}
num_articles = len(modified_articles)

for i in range(num_articles):
    modified_article = str(modified_articles[i])
    reference_summary = str(data['summaries'][i])

    scores = rouge.get_scores(modified_article, reference_summary)
    for metric in scores[0].keys():
        for rouge_type in scores[0][metric].keys():
            if rouge_type not in total_scores:
                total_scores[rouge_type] = {metric: scores[0][metric][rouge_type]}
            else:
                if metric not in total_scores[rouge_type]:
                    total_scores[rouge_type][metric] = scores[0][metric][rouge_type]
                else:
                    total_scores[rouge_type][metric] += scores[0][metric][rouge_type]

average_scores = {}
for rouge_type in total_scores.keys():
    average_scores[rouge_type] = {
        metric: total_scores[rouge_type][metric] / num_articles
        for metric in total_scores[rouge_type].keys()
    }
print("Average ROUGE scores for all articles:")
print(average_scores)

import sys
import statistics
from rouge import Rouge

sys.setrecursionlimit(10**6)

rouge = Rouge()
score_list = []

for i in range(len(modified_articles)):
    modified_article = str(modified_articles[i])
    reference_summary = str(data['summaries'][i])
    scores = rouge.get_scores(modified_article, reference_summary)
    score_list.append(scores[0])

rouge_1_scores = [score['rouge-1']['f'] for score in score_list]
rouge_1_median = statistics.median(rouge_1_scores)
rouge_1_std_dev = statistics.stdev(rouge_1_scores)

rouge_2_scores = [score['rouge-2']['f'] for score in score_list]
rouge_2_median = statistics.median(rouge_2_scores)
rouge_2_std_dev = statistics.stdev(rouge_2_scores)

rouge_l_scores = [score['rouge-l']['f'] for score in score_list]
rouge_l_median = statistics.median(rouge_l_scores)
rouge_l_std_dev = statistics.stdev(rouge_l_scores)

print(f"ROUGE-1 Median: {rouge_1_median}")
print(f"ROUGE-1 Standard Deviation: {rouge_1_std_dev}")

print(f"ROUGE-2 Median: {rouge_2_median}")
print(f"ROUGE-2 Standard Deviation: {rouge_2_std_dev}")

print(f"ROUGE-L Median: {rouge_l_median}")
print(f"ROUGE-L Standard Deviation: {rouge_l_std_dev}")

