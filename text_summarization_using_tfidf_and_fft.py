# -*- coding: utf-8 -*-
"""Text Summarization using tfidf and fft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ncpNSO0nSLjdNA-yhF19wO-W8mC1P_bp
"""

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

mkdir -p /root/.kaggle/

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download pariza/bbc-news-summary

! unzip bbc-news-summary.zip

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

articles_path = "/content/bbc news summary/BBC News Summary/News Articles"
summaries_path = "/content/bbc news summary/BBC News Summary/Summaries"
categories_list = os.listdir(articles_path)

import glob as glob


def read_file(articles_path, summaries_path, categories_list, encoding = "ISO-8859-1"):
    articles = []
    summaries = []
    categories = []
    for category in categories_list:
        article_paths = glob.glob(os.path.join(articles_path, category, '*.txt'), recursive = True)
        summary_paths = glob.glob(os.path.join(summaries_path, category, '*.txt'), recursive = True)

        print(f'found {len(article_paths)} file in articles/{category} folder, {len(summary_paths)} file in summaries/{category}')

        if len(article_paths) != len(summary_paths):
            print("number of files is not equal")
            return
        for file in range(len(article_paths)):
            categories.append(category)
            with open(article_paths[file], mode = 'r', encoding = encoding ) as files:
                articles.append(files.read())
            with open(summary_paths[file], mode = 'r', encoding = encoding) as files:
                summaries.append(files.read())

    print(f'total {len(articles)} file in articles folder and {len(summaries)} files in summaries folder')
    return articles, summaries, categories

articles, summaries, categories = read_file(articles_path, summaries_path, categories_list)

data = pd.DataFrame({'articles': articles, 'summaries': summaries,
                    'categories': categories})

data.head()

data.isnull().sum()

import re
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text

def sentence_tokenizer(article):
    sentences = re.split(r'(?<=[.!?])\s+', article)
    return sentences
preprocessed_sentences = []
for article in data['articles']:
    preprocessed_article = preprocess_text(article)
    sentences = sentence_tokenizer(preprocessed_article)
    preprocessed_sentences.extend(sentences)
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(data['articles'])
print(tfidf_matrix)



import numpy as np
count = 0
modified_articles = []
with open('modified_articles.txt', 'w', encoding='utf-8') as file:
  for i, article in enumerate(data['articles']):
      sentences = sentence_tokenizer(article)
      article_sentences = []
      modified_article = []
      sentence_sums = []
      for sentence in sentences:
          sentence_scores = []

          for word in sentence.split():
              if word in tfidf_vectorizer.vocabulary_:
                  word_index = tfidf_vectorizer.vocabulary_[word]
                  word_scores = tfidf_matrix[:, word_index].toarray()
                  sentence_scores.extend(word_scores)

          if sentence_scores:
              sentence_scores = np.array(sentence_scores)
              power_spectrum = np.abs(np.fft.fft(sentence_scores))**2
              sentence_sum = np.sum(power_spectrum)
              sentence_sums.append(sentence_sum)
      sorted_indices = np.argsort(sentence_sums)[::-1]

      selected_indices = sorted_indices[:11]

      for index in selected_indices:
          selected_sentence = sentences[index]
          modified_article.append(selected_sentence)

      modified_articles.append(modified_article)
      count += len(modified_article)

      file.write("Article {}:\n".format(i+1))
      for sentence in modified_article:
          file.write(sentence + "\n")

  '''          if sentence_sum > 0.5:
                  article_sentences.append((sentence, sentence_sum))
                  modified_article.append(sentence)
          modified_article_str = " ".join(modified_article)
          modified_articles.append(modified_article_str)
  '''

import numpy as np
import time

count = 0
modified_articles = []
sentence_processing_times = []

with open('modified_articles.txt', 'w', encoding='utf-8') as file, open('sentence_processing_times.txt', 'w', encoding='utf-8') as time_file:
    for i, article in enumerate(data['articles']):
        sentences = sentence_tokenizer(article)
        article_sentences = []
        modified_article = []
        sentence_sums = []

        for sentence in sentences:
            start_time = time.time()  # Start measuring time for processing the current sentence

            sentence_scores = []
            for word in sentence.split():
                if word in tfidf_vectorizer.vocabulary_:
                    word_index = tfidf_vectorizer.vocabulary_[word]
                    word_scores = tfidf_matrix[:, word_index].toarray()
                    sentence_scores.extend(word_scores)
            if sentence_scores:
                sentence_scores = np.array(sentence_scores)
                power_spectrum = np.abs(np.fft.fft(sentence_scores)) ** 2
                sentence_sum = np.sum(power_spectrum)
                sentence_sums.append(sentence_sum)

            end_time = time.time()  # Stop measuring time for processing the current sentence
            processing_time = end_time - start_time
            sentence_processing_times.append(processing_time)

            # Write the sentence index and time taken to the 'sentence_processing_times.txt' file
            time_file.write("Sentence {}: Time taken: {} seconds\n".format(len(sentence_processing_times), processing_time))

        sorted_indices = np.argsort(sentence_sums)[::-1]
        selected_indices = sorted_indices[:10]
        for index in selected_indices:
            selected_sentence = sentences[index]
            modified_article.append(selected_sentence)

        modified_articles.append(modified_article)
        count += len(modified_article)

        file.write("Article {}:\n".format(i + 1))
        for sentence in modified_article:
            file.write(sentence + "\n")

from rouge import Rouge
reference_summaries = data['summaries']
rouge = Rouge()
scores = []

for i, modified_article in enumerate(modified_articles):
    reference_summary = reference_summaries[i]

    modified_article_str = ' '.join(modified_article)
    reference_summary_str = reference_summary

    score = rouge.get_scores(modified_article_str, reference_summary_str)
    scores.append(score)
for i, score in enumerate(scores):
    print(f"Article {i+1}:")
    print("ROUGE-1: ", score[0]["rouge-1"])
    print("ROUGE-2: ", score[0]["rouge-2"])
    print("ROUGE-L: ", score[0]["rouge-l"])
    print()
