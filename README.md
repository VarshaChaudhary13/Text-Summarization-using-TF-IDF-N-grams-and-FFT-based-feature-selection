This research conducted a thorough examination of 1-gram, 2-gram, and 3-gram text summarization techniques across BBC articles, WikiHow guides, and the DUC 2004 dataset. The results revealed distinct performance variations across content domains. Specifically, within WikiHow guides, 2-gram and 3-gram models consistently outperformed 1-gram models, showcasing an average ROUGE score increase of approximately 7% and 9%, respectively, in capturing nuanced information. Conversely, the 1-gram models exhibited superior summarization proficiency with BBC news articles, achieving an average ROUGE score approximately 20% higher compared to the other n-gram models.
This study underscores the critical significance of aligning summarization techniques with dataset characteristics. While 1-gram models demonstrate effectiveness in specific contexts, the adaptability of 2-gram and 3-gram models is evident in capturing intricate contextual dependencies. Notably, across WikiHow and certain DUC 2004 articles, the 2-gram and 3-gram models showcased an average ROUGE score increment of around 12% and 14%, respectively, over the 1-gram models.
These insights emphasize the necessity for a nuanced approach in text summarization strategies. Recognizing the impact of content nuances on model performance advocates for meticulous model selection to ensure optimal summarization outcomes in diverse information retrieval scenarios.
In conclusion, this study significantly advances text summarization methodologies by highlighting the essentiality of discerning model selection based on content types across various datasets. This emphasis fosters improved information condensation strategies applicable across diverse applications.
